{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM,Bidirectional\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,mean_squared_error\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical,plot_model\n",
    "from sklearn.utils import resample\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=pd.read_csv(\"/Volumes/Maxtor/fakeproteins/fakeresmerge2.csv\",delimiter=\",\")\n",
    "data=pd.read_csv(\"/Volumes/Maxtor/firstrain.csv\",delimiter=\",\")\n",
    "init=data['peptides'].values\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(init)\n",
    "x= np.asarray(tokenizer.texts_to_sequences(init))\n",
    "xo=x\n",
    "y=np.asarray(data['NB'])\n",
    "x=to_categorical(x,num_classes=22)\n",
    "y=to_categorical(y,num_classes=2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectOutputAndTarget(Callback):\n",
    "    def __init__(self):\n",
    "        super(CollectOutputAndTarget, self).__init__()\n",
    "        self.targets = []  # collect y_true batches\n",
    "        self.outputs = []  # collect y_pred batches\n",
    "        self.inputs= []\n",
    "\n",
    "        # the shape of these 2 variables will change according to batch shape\n",
    "        # to handle the \"last batch\", specify `validate_shape=False`\n",
    "        self.var_y_true = tf.Variable(0., validate_shape=False)\n",
    "        self.var_y_pred = tf.Variable(0., validate_shape=False)\n",
    "        self.var_x = tf.Variable(0., validate_shape=False)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # evaluate the variables and save them into lists\n",
    "        self.targets.append(K.eval(self.var_y_true))\n",
    "        self.outputs.append(K.eval(self.var_y_pred))\n",
    "        if len(self.inputs)>1:\n",
    "            print(np.array_equal(self.inputs[-1],self.validation_data[0]))\n",
    "        self.inputs.append(self.validation_data[0])\n",
    "        #print(K.eval(self.var_y_true))\n",
    "        #print(self.inputs)\n",
    "        #print('Pred:')\n",
    "        \n",
    "class AccLossPlotter(Callback):\n",
    "    \"\"\"Plot training Accuracy and Loss values on a Matplotlib graph. \n",
    "    The graph is updated by the 'on_epoch_end' event of the Keras Callback class\n",
    "    # Arguments\n",
    "        graphs: list with some or all of ('acc', 'loss')\n",
    "        save_graph: Save graph as an image on Keras Callback 'on_train_end' event \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graphs=['acc', 'loss'], save_graph=True):\n",
    "        self.graphs = graphs\n",
    "        self.num_subplots = len(graphs)\n",
    "        self.save_graph = save_graph\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.epoch_count = 0\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_count += 1\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        epochs = [x for x in range(self.epoch_count)]\n",
    "\n",
    "        count_subplots = 0\n",
    "        \n",
    "        if 'acc' in self.graphs:\n",
    "            count_subplots += 1\n",
    "            plt.subplot(self.num_subplots, 1, count_subplots)\n",
    "            plt.title('Accuracy')\n",
    "            #plt.axis([0,100,0,1])\n",
    "            plt.plot(epochs, self.val_acc, color='r')\n",
    "            plt.plot(epochs, self.acc, color='b')\n",
    "            plt.ylabel('accuracy')\n",
    "\n",
    "            red_patch = mpatches.Patch(color='red', label='Test')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Train')\n",
    "\n",
    "            plt.legend(handles=[red_patch, blue_patch], loc=4)\n",
    "\n",
    "        if 'loss' in self.graphs:\n",
    "            count_subplots += 1\n",
    "            plt.subplot(self.num_subplots, 1, count_subplots)\n",
    "            plt.title('Loss')\n",
    "            #plt.axis([0,100,0,5])\n",
    "            plt.plot(epochs, self.val_loss, color='r')\n",
    "            plt.plot(epochs, self.loss, color='b')\n",
    "            plt.ylabel('loss')\n",
    "\n",
    "            red_patch = mpatches.Patch(color='red', label='Test')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Train')\n",
    "\n",
    "            plt.legend(handles=[red_patch, blue_patch], loc=4)\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        if self.save_graph:\n",
    "            plt.savefig('training_acc_loss.png')\n",
    "plot_losses = AccLossPlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carloswertcarvajal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/carloswertcarvajal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(None, 13))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def create_network(neurons=1):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(10,input_dim=(none,13)))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    adam = Adam(lr=0.01, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "neural_network = KerasClassifier(build_fn=create_network, \n",
    "                                 epochs=100, \n",
    "                                 batch_size=10, \n",
    "                                 verbose=0)\n",
    "neurons = [25, 28, 30, 64, 70]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid=GridSearchCV(estimator=neural_network, param_grid=param_grid, n_jobs=-1,scoring='roc_auc')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "#filepath=\"weights.best.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint,plot_losses]\n",
    "#history=model.fit(x_train, y_train, batch_size=16, epochs=10,validation_split=0.1,callbacks=callbacks_list)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "#y_pred=model.predict(x_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': 1,\n",
       " 'n': 2,\n",
       " 'd': 3,\n",
       " '_': 4,\n",
       " 'l': 5,\n",
       " 'i': 6,\n",
       " 'e': 7,\n",
       " 's': 8,\n",
       " 'p': 9,\n",
       " 'r': 10,\n",
       " 'a': 11,\n",
       " 'y': 12,\n",
       " 'g': 13,\n",
       " 'v': 14,\n",
       " 'k': 15,\n",
       " 'f': 16,\n",
       " 'q': 17,\n",
       " 't': 18,\n",
       " 'h': 19,\n",
       " 'w': 20,\n",
       " 'c': 21}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist=np.array(history.history['loss'])\n",
    "plt.plot(hist)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.383246162864897, 0.84]\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "plot_model(model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 13 13 30\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.argmax(model.predict(x_test,batch_size=16),axis=1)\n",
    "y_testp=np.argmax(y_test,axis=1)\n",
    "tn, fp, fn, tp=confusion_matrix(y_testp,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527 9 1 1535\n",
      "0.9967447916666666\n"
     ]
    }
   ],
   "source": [
    "data2=pd.read_csv(\"/Volumes/Maxtor/firstrain.csv\",delimiter=\",\")\n",
    "init=data['peptides'].values\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(init)\n",
    "x= np.asarray(tokenizer.texts_to_sequences(init))\n",
    "xo=x\n",
    "y=np.asarray(data['NB'])\n",
    "x=to_categorical(x,num_classes=22)\n",
    "y=to_categorical(y,num_classes=2)\n",
    "y_pred=np.argmax(model.predict(x,batch_size=16),axis=1)\n",
    "y_testp=np.argmax(y,axis=1)\n",
    "tn, fp, fn, tp=confusion_matrix(y_testp,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print((tp+tn)/(fp+fn+tp+tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.831571 using {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(20, output_dim=128,input_length=9))\n",
    "model.add(Bidirectional(LSTM(100,input_shape=(9,22))))\n",
    "model.add(Dense(64, input_dim=128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "adam = Adam(lr=0.01, epsilon=None, decay=0.0, amsgrad=False)\n",
    "BATCH_SIZE=40\n",
    "def datGen():\n",
    "    while True:\n",
    "        batchX=np.zeros(((BATCH_SIZE,)+x_train.shape[1:] ))\n",
    "        batchY= np.zeros( (BATCH_SIZE,2 ))\n",
    "        nElems=0\n",
    "        for i in range(x_train.shape[0]):\n",
    "          batchX[nElems]= x_train[i]\n",
    "          batchY[nElems]= y_train[i]\n",
    "          nElems+=1\n",
    "          if nElems==BATCH_SIZE:\n",
    "            yield (batchX, batchY)\n",
    "            nElems=0\n",
    "valdata=next(datGen())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "cbk = CollectOutputAndTarget()\n",
    "fetches = [tf.assign(cbk.var_y_true, model.targets[0], validate_shape=False),\n",
    "           tf.assign(cbk.var_y_pred, model.outputs[0], validate_shape=False)]\n",
    "model._function_kwargs = {'fetches': fetches}\n",
    "#history=model.fit_generator(datGen(), steps_per_epoch=1,\n",
    "          #epochs=100,callbacks=[cbk],validation_data=valdata)\n",
    "history=model.fit(valdata[0],valdata[1], batch_size=BATCH_SIZE, epochs=100,validation_data=valdata,callbacks=[cbk])\n",
    "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/Volumes/Maxtor/firstrain.csv\",delimiter=\",\")\n",
    "data = data.drop_duplicates(subset='peptides').reset_index(drop=True)\n",
    "init=data['peptides'].values\n",
    "c=0\n",
    "for word in init:\n",
    "    init[c]=word\n",
    "    c+=1\n",
    "x=np.asarray(init)\n",
    "y=np.asarray(data['NB'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "trains=pd.DataFrame(data={'peptides':x_train, 'NB': y_train})\n",
    "test=pd.DataFrame(data={'peptides':x_test, 'NB': y_test})\n",
    "trains.to_csv(\"/Volumes/Maxtor/firsttraintrain5.csv\",index=False)\n",
    "test.to_csv(\"/Volumes/Maxtor/testproteins5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(init[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
